{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание\n",
    "\n",
    "Цель: изучить применение методов оптимизации для решения задачи классификации\n",
    "Описание задания:\n",
    "В домашнем задании необходимо применить полученные знания в теории оптимизации и машинном обучении для реализации логистической регрессии.\n",
    "Этапы работы:**\n",
    "\n",
    "    1. Загрузите данные. Используйте датасет с ирисами. Его можно загрузить непосредственно из библиотеки Sklearn. В данных оставьте только 2 класса: Iris Versicolor, Iris Virginica.\n",
    "    2. Самостоятельно реализуйте логистическую регрессию, без использования метода LogisticRegression из библиотеки. Можете использовать библиотеки pandas, numpy, math для реализации. Оформите в виде функции. *Оформите в виде класса с методами.\n",
    "    3. Реализуйте метод градиентного спуска. Обучите логистическую регрессию этим методом. Выберете и посчитайте метрику качества. Метрика должна быть одинакова для всех пунктов домашнего задания. Для упрощения сравнения выберете только одну метрику.\n",
    "    4. Повторите п. 3 для метода скользящего среднего (Root Mean Square Propagation, RMSProp).\n",
    "    5. Повторите п. 3 для ускоренного по Нестерову метода адаптивной оценки моментов (Nesterov–accelerated Adaptive Moment Estimation, Nadam).\n",
    "    6. Сравните значение метрик для реализованных методов оптимизации. Можно оформить в виде таблицы вида |метод|метрика|время работы| (время работы опционально). Напишите вывод.\n",
    "\n",
    "Для лучшего понимания темы и упрощения реализации можете обратиться к статье.\n",
    "\n",
    "Для получение зачета по этому домашнему заданию, минимально, должно быть реализовано обучение логистической регрессии и градиентный спуск."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# импорт библиотек\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загрузка датасета из sklearn\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>7.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>6.4</td>\n",
       "      <td>3.2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>5.5</td>\n",
       "      <td>2.3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>6.5</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.6</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)   \n",
       "50                7.0               3.2                4.7               1.4  \\\n",
       "51                6.4               3.2                4.5               1.5   \n",
       "52                6.9               3.1                4.9               1.5   \n",
       "53                5.5               2.3                4.0               1.3   \n",
       "54                6.5               2.8                4.6               1.5   \n",
       "\n",
       "    target  \n",
       "50       0  \n",
       "51       0  \n",
       "52       0  \n",
       "53       0  \n",
       "54       0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# преобразование датасета в pandas\n",
    "# df = pd.DataFrame(data=np.c_[dataset['data', 'feature_names'], dataset['target']], columns= dataset['feature_names'] + ['target']) # короткий способ\n",
    "df = pd.DataFrame(iris['data'], columns=iris['feature_names']) # загрузка данных\n",
    "df['target'] = iris.target # загрузка таргетов\n",
    "df['names'] = pd.Categorical.from_codes(iris.target, iris.target_names) # добавление названий\n",
    "df = df[df['target'].isin([1,2])] # убираю setosa\n",
    "df.target = df.target.replace(1, 0) # замена класса с 1 на 0\n",
    "df.target = df.target.replace(2, 1) # замена класса с 2 на 1\n",
    "df = df.drop(columns=['names']) # убираю названия\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# подготовка данных\n",
    "X = df.drop('target', axis = 1).values # данные\n",
    "y = df.target.values # целевые значения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = np.c_[np.ones(len(X)), X] # тренировался с добавлением единичных позиций в вектор, но в результате не стал использовать, т.к. точность с ними оказалась ниже"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# делю данные на тренировочную и тестовую выборки\n",
    "# по стуи, в задании это не требуется, убираю этот шаг\n",
    "# x_train, x_test, y_train, y_test = train_test_split(x ,y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция сигмоиды\n",
    "\n",
    "def sigmoid(z):    \n",
    "    y_proba = 1 / (1+np.exp(-z))\n",
    "    return y_proba\n",
    "\n",
    "# то же в краткой записи\n",
    "# sigmoid = lambda z: 1 / (1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# визуализация сигмоиды\n",
    "# делал для понимания\n",
    "# z = np.linspace(-10, 10, 1000)\n",
    "# plt.plot(z, sigmoid(z))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# задаём изначальный вес и биас\n",
    "# создаёт нулевую матрицу по количеству признаков\n",
    "# def k_0(n_features):\n",
    "#     w = np.zeros((1,n_features))\n",
    "#     b = 0\n",
    "#     return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# проверяем на количестве признаков\n",
    "# xt.shape[1]\n",
    "# w, b = k_0(xt.shape[1])\n",
    "# w, b\n",
    "# # выдаёт нулевую матрицу из 4-х столбцов по количеству признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# вычисляю z для сигмы\n",
    "# z = np.dot(xt.T, w)\n",
    "# z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаю функцию потерь\n",
    "def logloss(y, y_proba):\n",
    "    logloss_1 = np.sum(np.log(y_proba[y == 1] + 1e-30))\n",
    "    logloss_0 = np.sum(np.log(1 - y_proba[y == 0] + 1e-30))\n",
    "    logloss_total = -(logloss_0 + logloss_1) / len(y)\n",
    "    return logloss_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаю функцию градиента\n",
    "def grad_logloss(X, W, y):\n",
    "    y_proba = sigmoid(X @ W)\n",
    "    grad = X.T @ (y_proba - y)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 3.28848060e-15, -3.63598041e-15, -5.42066392e-16, -1.28397293e-15]),\n",
       " array([1., 1., 1., 1.]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# нормализация данных, без этого не работало и показывало полную ерунду\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X.mean(axis=0), X.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0\n",
      "weight: [0.02471525 0.01540399 0.03932118 0.04140646]\n",
      "0.6534814598101053\n",
      "accuracy: 0.9\n",
      "\n",
      "iteration: 25\n",
      "weight: [0.26227998 0.10355101 0.57930078 0.64312087]\n",
      "0.3402175815645784\n",
      "accuracy: 0.93\n",
      "\n",
      "iteration: 50\n",
      "weight: [0.29241447 0.04711189 0.84848468 0.96245999]\n",
      "0.26816231909758037\n",
      "accuracy: 0.94\n",
      "\n",
      "iteration: 75\n",
      "weight: [ 0.27971422 -0.02706686  1.03944636  1.19232669]\n",
      "0.23013375552561827\n",
      "accuracy: 0.95\n",
      "\n",
      "iteration: 100\n",
      "weight: [ 0.2527156  -0.1002897   1.19096275  1.37495354]\n",
      "0.20518728205118872\n",
      "accuracy: 0.95\n",
      "\n",
      "iteration: 125\n",
      "weight: [ 0.22054254 -0.16810862  1.31806069  1.52750598]\n",
      "0.18718107955459737\n",
      "accuracy: 0.95\n",
      "\n",
      "iteration: 150\n",
      "weight: [ 0.18684912 -0.22965156  1.42834473  1.65896679]\n",
      "0.17344976263131293\n",
      "accuracy: 0.95\n",
      "\n",
      "iteration: 175\n",
      "weight: [ 0.15325465 -0.28514352  1.52626117  1.77472136]\n",
      "0.16258448048313537\n",
      "accuracy: 0.95\n",
      "\n",
      "iteration: 200\n",
      "weight: [ 0.1205017  -0.3351427   1.61465676  1.87828446]\n",
      "0.15374983975236298\n",
      "accuracy: 0.96\n",
      "\n",
      "iteration: 225\n",
      "weight: [ 0.08892042 -0.38027376  1.69547208  1.97208823]\n",
      "0.14641214838134176\n",
      "accuracy: 0.96\n",
      "\n",
      "iteration: 250\n",
      "weight: [ 0.05863655 -0.42113189  1.77009271  2.05788996]\n",
      "0.14021204310237056\n",
      "accuracy: 0.96\n",
      "\n",
      "iteration: 275\n",
      "weight: [ 0.02967182 -0.45825149  1.83954413  2.13700269]\n",
      "0.13489777062259464\n",
      "accuracy: 0.96\n",
      "\n",
      "iteration: 300\n",
      "weight: [ 1.99498237e-03 -4.92099970e-01  1.90460757e+00  2.21043445e+00]\n",
      "0.13028735795067914\n",
      "accuracy: 0.96\n",
      "\n",
      "iteration: 325\n",
      "weight: [-0.02445111 -0.52308135  1.96589247  2.27897663]\n",
      "0.12624585667753072\n",
      "accuracy: 0.96\n",
      "\n",
      "iteration: 350\n",
      "weight: [-0.04973502 -0.55154317  2.02388398  2.34326243]\n",
      "0.12267101134897906\n",
      "accuracy: 0.96\n",
      "\n",
      "iteration: 375\n",
      "weight: [-0.07392874 -0.57778403  2.07897493  2.40380677]\n",
      "0.11948389377961158\n",
      "accuracy: 0.96\n",
      "\n",
      "iteration: 400\n",
      "weight: [-0.09710342 -0.60206066  2.13148824  2.46103434]\n",
      "0.11662259276849202\n",
      "accuracy: 0.96\n",
      "\n",
      "iteration: 425\n",
      "weight: [-0.11932714 -0.62459427  2.1816928   2.51529975]\n",
      "0.11403785113790182\n",
      "accuracy: 0.97\n",
      "\n",
      "iteration: 450\n",
      "weight: [-0.14066379 -0.64557592  2.2298151   2.56690235]\n",
      "0.11168998123760275\n",
      "accuracy: 0.97\n",
      "\n",
      "iteration: 475\n",
      "weight: [-0.16117274 -0.66517114  2.27604792  2.61609728]\n",
      "0.10954664141304288\n",
      "accuracy: 0.97\n",
      "\n",
      "iteration: 500\n",
      "weight: [-0.18090869 -0.68352379  2.3205568   2.66310394]\n",
      "0.10758120525778413\n",
      "accuracy: 0.97\n",
      "\n",
      "iteration: 525\n",
      "weight: [-0.19992192 -0.7007593   2.36348507  2.70811242]\n",
      "0.10577154705063803\n",
      "accuracy: 0.97\n",
      "\n",
      "iteration: 550\n",
      "weight: [-0.21825853 -0.71698735  2.40495778  2.7512886 ]\n",
      "0.1040991245162993\n",
      "accuracy: 0.97\n",
      "\n",
      "iteration: 575\n",
      "weight: [-0.23596072 -0.73230423  2.44508473  2.79277816]\n",
      "0.10254827734172514\n",
      "accuracy: 0.98\n",
      "\n",
      "iteration: 600\n",
      "weight: [-0.25306717 -0.74679464  2.48396293  2.83270978]\n",
      "0.10110568449053632\n",
      "accuracy: 0.98\n",
      "\n",
      "iteration: 625\n",
      "weight: [-0.26961329 -0.76053338  2.52167858  2.87119774]\n",
      "0.0997599399123891\n",
      "accuracy: 0.98\n",
      "\n",
      "iteration: 650\n",
      "weight: [-0.28563161 -0.77358668  2.55830865  2.90834399]\n",
      "0.09850121757451187\n",
      "accuracy: 0.98\n",
      "\n",
      "iteration: 675\n",
      "weight: [-0.30115198 -0.78601334  2.59392218  2.94423994]\n",
      "0.09732100461991369\n",
      "accuracy: 0.98\n",
      "\n",
      "iteration: 700\n",
      "weight: [-0.31620185 -0.79786573  2.6285814   2.97896785]\n",
      "0.09621188701257363\n",
      "accuracy: 0.98\n",
      "\n",
      "iteration: 725\n",
      "weight: [-0.33080652 -0.8091906   2.66234259  3.01260203]\n",
      "0.09516737600035712\n",
      "accuracy: 0.98\n",
      "\n",
      "iteration: 750\n",
      "weight: [-0.34498934 -0.82002979  2.69525685  3.04520986]\n",
      "0.0941817665985707\n",
      "accuracy: 0.98\n",
      "\n",
      "iteration: 775\n",
      "weight: [-0.35877188 -0.83042084  2.72737075  3.07685264]\n",
      "0.09325002139830324\n",
      "accuracy: 0.98\n",
      "\n",
      "iteration: 800\n",
      "weight: [-0.37217411 -0.84039752  2.75872683  3.10758629]\n",
      "0.09236767455712414\n",
      "accuracy: 0.98\n",
      "\n",
      "iteration: 825\n",
      "weight: [-0.38521455 -0.84999028  2.78936412  3.137462  ]\n",
      "0.09153075198943028\n",
      "accuracy: 0.98\n",
      "\n",
      "iteration: 850\n",
      "weight: [-0.39791041 -0.85922662  2.81931848  3.16652671]\n",
      "0.09073570464749349\n",
      "accuracy: 0.98\n",
      "\n",
      "iteration: 875\n",
      "weight: [-0.41027768 -0.86813147  2.84862296  3.19482363]\n",
      "0.08997935244824727\n",
      "accuracy: 0.98\n",
      "\n",
      "iteration: 900\n",
      "weight: [-0.4223313  -0.87672742  2.87730811  3.22239256]\n",
      "0.08925883690952933\n",
      "accuracy: 0.98\n",
      "\n",
      "iteration: 925\n",
      "weight: [-0.43408518 -0.88503503  2.9054022   3.24927032]\n",
      "0.08857158095216834\n",
      "accuracy: 0.98\n",
      "\n",
      "iteration: 950\n",
      "weight: [-0.44555233 -0.89307302  2.9329315   3.27549098]\n",
      "0.08791525462961954\n",
      "accuracy: 0.98\n",
      "\n",
      "iteration: 975\n",
      "weight: [-0.45674495 -0.90085848  2.95992042  3.30108615]\n",
      "0.08728774578586897\n",
      "accuracy: 0.98\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# реализую логистическую регрессию\n",
    "def log_reg(X, y, lr=0.001, iterations=1000, report_iter_frequency=50): # создаю функцию с указанными по умолчанию learning_rate и количеством итераций\n",
    "    m, n = X.shape # строки и столбцы в выборке\n",
    "    w = np.zeros(n) # задаю изначальный вес\n",
    "\n",
    "    for i in range(iterations):\n",
    "        grad = grad_logloss(X, w, y)\n",
    "        w -= lr*(grad) # обновление веса по градиенту сдвиг веса по радиенту, при тренировка расписывал текущий и следующий вес, но в финальной реализации решил упростить\n",
    "        \n",
    "        # считаю предсказание и точность\n",
    "        y_proba = sigmoid(np.dot(X, w))\n",
    "        y_class = np.where(y_proba >= 0.5, 1, 0) # предсказание класса\n",
    "        accuracy = (y_class == y).sum() / len(y) # считаю точность\n",
    "        \n",
    "        if i % report_iter_frequency == 0:\n",
    "            print(f\"iteration: {i}\")\n",
    "            print(f\"weight: {w}\")\n",
    "            print(logloss(y, y_proba))\n",
    "            print(f\"accuracy: {accuracy}\\n\")\n",
    "    return\n",
    "log_reg(X, y, 0.001, 1000, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# полный тренировочный набор логистической регрессии\n",
    "\n",
    "# def log_reg(X, y, lr=0.001, iterations=1000): # создаю функцию с указанными по умолчанию learning_rate и количеством итераций\n",
    "#     m, n = X.shape # строки и столбцы в выборке\n",
    "#     #w = np.zeros(n)\n",
    "    \n",
    "#     np.random.seed(8)\n",
    "#     w = np.random.randn(X.shape[1])\n",
    "    \n",
    "#     nw = w\n",
    "\n",
    "#     b = 0\n",
    "\n",
    "#     # next_w = w\n",
    "\n",
    "#     for i in range(iterations):\n",
    "#         grad = grad_logloss(X, w, y)\n",
    "#         cw = nw\n",
    "#         nw = cw - lr*grad\n",
    "#         w -= lr*(grad) # обновление веса по градиенту сдвиг веса по радиенту\n",
    "        \n",
    "#         # считаю предсказание и точность\n",
    "#         y_proba = sigmoid(np.dot(X, w))\n",
    "#         y_class = np.where(y_proba >= 0.5, 1, 0) # предсказание класса\n",
    "#         accuracy = (y_class == y).sum() / len(y)\n",
    "        \n",
    "#         if i % 10 == 0:\n",
    "#             print(f\"{cw} | {nw}\")\n",
    "#             print(accuracy)\n",
    "#     return\n",
    "\n",
    "        # cur_w = next_w\n",
    "        # grad = grad_logloss(X, y, w) / m # реализация градиента через функциию\n",
    "        # next_w = cur_w - lr*grad # движение по градиенту вниз\n",
    "        # y_proba = sigmoid(np.dot(X, next_w))\n",
    "        # y_class = np.where(y_proba >= 0.5, 1, 0)\n",
    "\n",
    "        # print(y_proba)\n",
    "        \n",
    "        # if i % 5 == 0: # проверка каждые 5 итераций\n",
    "        #     print(f\"Итерация: {i}\")\n",
    "        #     y_proba = sigmoid(np.dot(x, next_w))\n",
    "        #     y_class = np.where(y_proba >= 0.5, 2, 1)\n",
    "        #     accuracy = (y_class == y).sum() / len(y)\n",
    "        #     # print(f\"Logloss {logloss(y, y_proba)}\")\n",
    "        #     print(f\"{y_proba}\")\n",
    "        #     print(f\"Accuracy {accuracy}\")\n",
    "        #     print(f'\\n') \n",
    "\n",
    "\n",
    "        # реализация градиента построчно\n",
    "        # z = np.dot(x, w) + b # рассчитывает аргумент для сигмоиды\n",
    "        # y_proba = sigmoid(z) # рассчитывает y_proba\n",
    "        # grad_z = y_proba - y # рассчитывает сдвиг y относительно реального y\n",
    "        # grad = np.dot(x.T, grad_z) #/ m  # рассчитывает градиент\n",
    "        # w -= lr*(grad) # сдвиг веса по радиенту\n",
    "        # b -= lr*np.sum((y_proba - y) /m) # сдвиг биаса\n",
    "        # много всяких print для тренировки\n",
    "        # print(x.shape)\n",
    "        # print(z)\n",
    "        # print(y_proba)\n",
    "        # print(\"weight\", w)\n",
    "        # print(\"cur_w\", cur_w)\n",
    "        # print(\"next_w\", next_w)\n",
    "        # print(y_class)\n",
    "        # print(\"bias\", b)\n",
    "        # print(grad)\n",
    "        # print(grad1)\n",
    "        # print(m)\n",
    "    # return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic_regression(X, y, alpha=0.01, num_iterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# из ноутбука с доп. материалами\n",
    "\n",
    "# eps = 0.001\n",
    "\n",
    "# # первоначальное точка\n",
    "# np.random.seed(8)\n",
    "# W = np.random.randn(X.shape[1])\n",
    "\n",
    "# # размер шага (learning rate)\n",
    "# learning_rate = 0.001\n",
    "\n",
    "# next_W = W\n",
    "\n",
    "# # количество итерация \n",
    "# n = 100\n",
    "# for i in range(n):\n",
    "#     cur_W = next_W\n",
    "\n",
    "#     # движение в негативную сторону вычисляемого градиента\n",
    "#     next_W = cur_W - learning_rate * grad_logloss(X, W, y)\n",
    "\n",
    "#     # остановка когда достигнута необходимая степень точности\n",
    "#     if np.linalg.norm(cur_W - next_W) <= eps:\n",
    "#         break\n",
    "\n",
    "#     if i % 10 == 0:\n",
    "#         # print(f\"Итерация: {i}\")\n",
    "#         # print(f\"Текущая точка {cur_W}| Следующая точка {next_W}\")\n",
    "#         print(f\"{cur_W} | {next_W}\")\n",
    "#         y_proba = sigmoid(X @ next_W)\n",
    "#         y_class = np.where(y_proba >= 0.5, 1, 0)\n",
    "#         # print(y_class)\n",
    "#         accuracy = (y_class == y).sum() / len(y)\n",
    "#         # print(f\"Logloss {logloss(y, y_proba)}\")\n",
    "#         # print(grad_logloss(X, W, y))\n",
    "#         print(f\"Accuracy {accuracy}\")\n",
    "#         print(\"--------------------------------------------------------\") \n",
    "\n",
    "#         # visualize(next_W)   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ссылка на полезную статью в kaggle https://www.kaggle.com/code/sagira/logistic-regression-math-behind-without-sklearn/notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
